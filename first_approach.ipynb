{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime,timedelta\n",
    "import csv\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: generate a list of task with the people on it and the deadline of each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's just try to determine the duration of a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As no dataset lets generate our own one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task : desc,type,deadline,technologies,team,duration(the output that we try to guess)\n",
    "#start: date of the task creation\n",
    "#type : \"bug\", \"feature\", \"refactor\", \"test\", \"documentation\"\n",
    "#technologies : \"python\", \"java\", \"c++\", \"c#\", \"javascript\", \"ruby\", \"php\", \"html\", \"css\", \"sql\", \"nosql\"\n",
    "#team : number of people in the team\n",
    "#priority : 0,1,2(0 :\"low\", 1:\"medium\", 2:\"high\")\n",
    "#duration : in days\n",
    "#deadline : a date in the future\n",
    "#desc : a string\n",
    "\n",
    "#generate a random but credible dataset of task\n",
    "\n",
    "def generate_dataset(sample):\n",
    "    \n",
    "    fake = Faker()\n",
    "    tasks = []\n",
    "    for i in range(sample):\n",
    "        min_duration = 1\n",
    "        task = {}\n",
    "        task[\"desc\"] = fake.text()\n",
    "        task[\"type\"] = random.choice([\"bug\", \"feature\", \"refactor\", \"test\", \"documentation\"])\n",
    "        task[\"start\"] = (datetime.now()+ timedelta(days=random.randint(0,3))).strftime(\"%Y-%m-%d\")\n",
    "        #documentation tasks are only about documentation\n",
    "        if task[\"type\"] == \"documentation\":\n",
    "            task[\"technologies\"] = [\"doc\"]\n",
    "        else:\n",
    "            task[\"technologies\"] = random.sample([\"python\", \"java\", \"c++\", \"c#\", \"web\", \"sql\", \"nosql\"], k=1)\n",
    "            # task[\"technologies\"] = random.sample([\"python\", \"java\", \"c++\", \"c#\", \"web\", \"sql\", \"nosql\",], k=random.randint(1, 2))\n",
    "        task[\"team\"] = random.randint(1, 10)\n",
    "        task[\"priority\"] = random.randint(0, 2)\n",
    "        # if task[\"team\"]< len(task[\"technologies\"]):\n",
    "        #     min_duration += (len(task[\"technologies\"]) - task[\"team\"])*5 #case more technologies than team members\n",
    "        if \"c++\" in task[\"technologies\"]:\n",
    "            min_duration += 1 #because c++ you know...\n",
    "        if \"documentation\" not in task[\"type\"]:\n",
    "            min_duration += 5 #documentation is easier\n",
    "        if len(task[\"desc\"]) > 100:\n",
    "            min_duration += len(task[\"desc\"])//30 #we can imagine than longer description = more time\n",
    "        \n",
    "        \n",
    "        #task[\"deadline\"] should be at least min_duration days in the future\n",
    "        start_date = datetime.now()\n",
    "        min_deadline_date = start_date + timedelta(days=min_duration)\n",
    "        #max_deadline change with the priority\n",
    "        if task[\"priority\"] == 0:\n",
    "            max_duration = min_duration + 15 #low priority tasks are less urgent\n",
    "        elif task[\"priority\"] == 1:\n",
    "            max_duration = min_duration + 10\n",
    "        else:\n",
    "            max_duration = min_duration + 5 #high priority tasks are more urgent\n",
    "        max_deadline_date = start_date + timedelta(days=max_duration)\n",
    "        task[\"deadline\"] = fake.date_time_between_dates(datetime_start=min_deadline_date, datetime_end=max_deadline_date).strftime(\"%Y-%m-%d\")\n",
    "        task[\"duration\"] = random.randint(min_duration, max_duration)\n",
    "        tasks.append(task)\n",
    "    with open(\"./data/tasks.csv\", \"w\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"desc\", \"type\", \"start\",\"deadline\", \"technologies\", \"team\",\"priority\", \"duration\"])\n",
    "        writer.writeheader()\n",
    "        for task in tasks:\n",
    "            writer.writerow(task)\n",
    "    return tasks\n",
    "\n",
    "generate_dataset(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/tasks.csv\")\t\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "#encode the type (one to many)\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=[\"type\"])\n",
    "\n",
    "#encode the technologies (many to many)\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=[\"technologies\"])\n",
    "\n",
    "#encode the start date\n",
    "df_encoded['start'] = pd.to_datetime(df_encoded['start'])\n",
    "df_encoded['start'] = df_encoded['start'].map(datetime.toordinal)\n",
    "#encode the deadline\n",
    "df_encoded['deadline'] = pd.to_datetime(df_encoded['deadline'])\n",
    "df_encoded['deadline'] = df_encoded['deadline'].map(datetime.toordinal)\n",
    "\n",
    "#ensure the numeric value of start and deadline\n",
    "\n",
    "df_encoded['start'] = pd.to_numeric(df_encoded['start'])\n",
    "df_encoded['deadline'] = pd.to_numeric(df_encoded['deadline'])\n",
    "\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "df_without_desc  = df_encoded.drop(columns=[\"desc\"]) #cause too hard for the correlation matrix to generate\n",
    "corr_matrix = df_without_desc.corr()\n",
    "print(corr_matrix)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix,annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlation with the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between every feature and the duration\n",
    "correlation = corr_matrix[\"duration\"].sort_values(ascending=False)\n",
    "correlation = correlation[correlation.index != \"duration\"]\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a model to determine the duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters :  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 30}\n",
      "Final RMSE :  1.382389236069205\n",
      "Final RMSE scores :  [1.58113883 1.14345966 0.98234414 0.8093207  0.82764727 1.39014388\n",
      " 1.07354553 1.11915146 1.24599358 1.38744369]\n",
      "Final RMSE scores mean :  1.1560188739974637\n",
      "Final RMSE scores std :  0.23742443631950816\n"
     ]
    }
   ],
   "source": [
    "#split data between training and test sets\n",
    "\n",
    "X = df_encoded.drop(columns=[\"duration\", \"desc\"])\n",
    "y = df_encoded[\"duration\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#lets use decision tree regressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "\n",
    "\n",
    "#Function in order to round the prediction to the nearest integer, in order to be use in the GridSearchCV and cross validation\n",
    "def custom_mse(y_true, y_pred):\n",
    "    y_pred_rounded = np.round(y_pred).astype(int)\n",
    "    return mean_squared_error(y_true, y_pred_rounded)\n",
    "\n",
    "# Wrap the custom scorer with make_scorer\n",
    "custom_scorer = make_scorer(custom_mse, greater_is_better=False)\n",
    "\n",
    "\n",
    "#grid search\n",
    "param_grid = [\n",
    "    {'max_depth': [2,3,4,5,10,15,20,30], 'min_samples_split': [2,3,4,5,10,15,20,30], 'min_samples_leaf': [1,2,5,10,15,20,30]}\n",
    "]\n",
    "grid_search = GridSearchCV(tree_reg, param_grid, cv=5, scoring=custom_scorer, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters : \", grid_search.best_params_)\n",
    "#final model\n",
    "final_model = grid_search.best_estimator_\n",
    "final_predictions = final_model.predict(X_test)\n",
    "#round predictions to the nearest integer because duration is an integer\n",
    "final_predictions = np.round(final_predictions).astype(int)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "print(\"Final RMSE : \", final_rmse)\n",
    "\n",
    "#cross validation\n",
    "scores = cross_val_score(final_model, X_train, y_train, scoring=custom_scorer, cv=10)\n",
    "final_rmse_scores = np.sqrt(-scores)\n",
    "print(\"Final RMSE scores : \", final_rmse_scores)\n",
    "print(\"Final RMSE scores mean : \", final_rmse_scores.mean())\n",
    "print(\"Final RMSE scores std : \", final_rmse_scores.std())\n",
    "\n",
    "#now lets put the rounded predict duration in the dataset\n",
    "\n",
    "df_encoded[\"predicted_duration_with_tree\"] = np.round(final_model.predict(X)).astype(int)\n",
    "df_encoded[\"diff_with_tree\"] = df_encoded[\"duration\"] - df_encoded[\"predicted_duration\"]\n",
    "df_encoded[\"diff_with_tree\"] = df_encoded[\"diff_with_tree\"].abs()\n",
    "#save it to a csv\n",
    "df_encoded.to_csv(\"./data/tasks_with_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
